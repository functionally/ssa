\documentclass[12pt]{article}
%\usepackage{amsmath}
\usepackage{graphicx}
%\usepackage{enumerate}
%\usepackage{natbib} %comment out if you do not have the package
\usepackage{url} % not crucial - just used below for the URL 

\usepackage[utf8]{inputenc}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[english]{babel}
\usepackage{bbm}
\usepackage{csquotes}
\usepackage{relsize}
\usepackage{todonotes}

\usepackage[
backend=biber,
style=alphabetic,
sorting=nyt
]{biblatex}

\addbibresource{sequential-sensitivity.bib}


%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{0}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%


\begin{document}

%\bibliographystyle{natbib}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\if0\blind
{
  \title{\bf Adaptively Sampling Using Regional Sensitivities}
  \author{
    Brian\ W.\ Bush \\
    National Renewable Energy Laboratory, \\ \\
    Joanne Wendelberger \\
    Los Alamos National Laboratory, \\ \\
    Rebecca Hanes \\
    National Renewable Energy Laboratory
  }
  \maketitle
} \fi

\if1\blind
{
  \bigskip
  \bigskip
  \bigskip
  \begin{center}
    {\LARGE\bf Adaptively Sampling Using Regional Sensitivities}
\end{center}
  \medskip
} \fi

\bigskip
\begin{abstract}
Inspired by the well-established variance-based methods for global sensitivity analysis, we develop a local total sensitivity index that decomposes the global total sensitivity conditions by independent variables. We employ this local sensitivity index in a new method of experimental design that sequentially and adaptively samples the domain of a multivariate function according to local contributions to the global variance. The method is demonstrated on a nonlinear toy model that has a three-dimensional domain and a three-dimensional codomain, but also on a complex, high-dimensional simulation of the production of bioproducts from biomass. \todo{\small 100 or fewer words.}
\end{abstract}

\noindent%
{\it Keywords: adaptive sampling; variance-based; regional; local; sensitivity; uncertainty} \todo{\small 3 to 6 keywords, that do not appear in the title}
\vfill

\newpage
%%%% FIXME: Remove comment before submitting.
%%%%\spacingset{2} % DON'T change the spacing!


\section{Introduction}

Computer experiments employing complex simulations, which typically involve dozens or hundreds of independent and dependent variables, pose daunting analytic challenges for developing defensible and robust insights about sensitivity, uncertainty, and system behavior. The high dimensionality of the input spaces necessitates economical exploration of input-parameter ranges and the similarly large output dimensions, often in the form of multivariate timeseries, stresses visual and statistical analyses. Well-established techniques for global sensitivity analysis, such as elementary-effects screening and variance-based first-order and total sensitivity indices \cite{saltelli_global_2008}, provide a solid foundation for exploring and characterizing these complex simulations, while local or regional methods such as Monte-Carlo filtering \cite{borgonovo_sensitivity_2016} allow targeted discrimination of sensitivities, but neither approach maps the landscape of contributions to sensitive dependence of outputs upon inputs. Understanding that landscape of sensitivity, however, may motivate deeper exploration and an iterative sequence of computer experiments to tease out significant details of the nonlinear responses in a simulation.

Indeed, a not uncommon practice in computer experimentation on complex models or simulations relies on an initial screening of sensitivities over a generously large set of variables, typically chosen according to the judgment of model developers and domain experts:\ the result of the broad, but computationally efficient, screening identifies a set of candidate input parameters for which a more rigorous and expensive global sensitivity analysis is warranted. Initial screening may use one-at-a-time variation of input parameters, while the subsequent global analysis typically relies on a uniformly-sampling space-filling design (e.g., fractional factorial, Latin hypercube, orthogonal array, or quasi-random sequence).\todo{\small Do we need to cite all of the bread-and-butter techniques mentioned in this paragraph?} Based on the results of the global sensitivity analysis and resources permitting, an analyst may execute subsequent sensitivity analyses that (1) narrow or broaden the range of variation in the independent variables, (2) add or remove candidate inputs from the set varied in the experiments, (3) add or remove output variables for evaluating sensitivities, or (4) increase sample size. Sample size is a particularly important issue because of the inverse relationship between number of input parameters varied in a sensitivity analysis and the tightness of the confidence intervals in the sensitivity indices (especially for total sensitivities or for higher than first-order sensitivities). Thus, subsequent iterations of a global sensitivity analysis may reduce the number of inputs considered, in order to tighten confidence intervals for the inconclusive results of previous iterations, and increase sample size. Early iterations might also correct overly broad or narrow ranges of input variation or discard outputs that do not exhibit interesting variation, as when a modeler or analyst belatedly realizes that an output variable exhibits a trivial dependence on input or trivially inconsequential variation. Furthermore, exploratory analysis and visualization of the raw simulation output, conditioned on the inputs, may reveal interesting patterns and dependencies---not apparent in the global sensitivity indices---of the output upon local regions of the input space:\ this may lead to focused ``side analyses'' that probe local regions or test specific hypotheses. If computational resources were unlimited, then none of this tedious and labor-intensive iteration would be necessary.

Our work here is motivated by the desire to automate much of the practical and predictably iterative process of model exploration and sensitivity analysis described in the previous paragraph, thus freeing the modeler and analyst to work at a higher level of abstraction in developing insights and testing hypotheses. In particular, we would like to semi-autonomously focus computational resources on the most potentially interesting regions of the input space using estimates of how much those regions contribute to global sensitivity indices. We envision a computational workflow where (1) an initial uniformly space-filling experimental design adaptively evolves to focus on regions of higher variance while not ignoring the lower-variance regions, (2) previously computed simulations are seamlessly incorporated into the adaptiving design, (3) users may at any point tune the intensity of focus on the higher-variance regions, and (4) users have the option to intervene to adjust the boundaries of the input space or the density of sampling. To this end, in this paper we develop a local decomposition of global variance-based sensitivity indices, formulated in a manner that enables iterative updating of the non-uniform space-filling sampling of the simulation output so that it focuses on higher-variance regions. Additionally, the sampling method is amenable to user interventions to tailor sampling to their interest.


\section{Background}

Saltelli \textit{et al.} \cite{saltelli_sensitivity_2004} \cite{saltelli_global_2008} provide a thorough and practical overviews of global variance-based sensitivity analysis methods, while \cite{saltelli_variance_2010} summarizes recomendations for designing experiments for and computing first-order and total sensitivity indices. Such recommendations arise from foundational work by a variety of researchers who investigated optimal experimental design \cite{jansen_analysis_1999} \cite{saltelli_variance_2010}, alternative computational recipes \cite{saltelli_variance_2010}, and efficient organization and minimization of model evaluations \cite{saltelli_making_2002} \cite{piano_new_2019} \cite{kucherenko_estimation_2012}.

Borgonovo and Plischke \cite{borgonovo_sensitivity_2016} broadly catalog the spectrum of sensitivity-analysis techniques, including not only global variance-based methods, but also local or regional non-variance methods such as Monte Carlo filtering \cite{wu_application_2017}. Domain specific reviews and assessments of sensitivity analysis are provided by Wagener and Pianosi \cite{wagener_what_2019} for earth-system modeling, Tian \cite{tian_review_2013} for building-energy analysis, Norton \cite{norton_introduction_2015} or Pianosi \textit{et al.} \cite{pianosi_sensitivity_2016} for environmental-simulation models, and Jadun \textit{et al} \cite{jadun_application_2017} or Inman \textit{et al.} \cite{inman_application_2018} for bioenergy supply-chain simulation.

Local and regional specializations of sensitivity analysis are discussed in Wei \cite{wei_regional_2014}, Wu \cite{wu_application_2017}, Spear, Grieb, and Shang \cite{spear_parameter_1994}, and Rose \textit{et al.} \cite{rose_parameter_1991}. Gamboa \textit{et al.} \cite{gamboa_sensitivity_2013} generalize sensitivity indices to multivariate output. Lu, Anderson-Cook, and Ahmed \cite{lu_non-uniform_2020} propose non-uniform space-filling (NUSF) experimental designs that control the density of sampling according to experimental objectives, while Bowman and Woods \cite{bowman_weighted_2013} define weighted space-filling (WSF) ones using a distance metric for the design space.

% Discuss Myers et al. \cite{myers_partitioning_2016}?


\section{Method}

We consider multivariate functions $\mathbf{Y} \left( \mathbf{X} \right)$ with $m$ independent variables $X_i$ and $n$ dependent variables $Y_j$. (Table~\ref{tab:notation} summarizes the notation used in this paper.) These functions may represent the input-output relationship of computer models (either deterministic or stochastic) or physical experiments.

We approach the problem of developing local versions of variable-based global sensitivity indices by expressing the standard global indices \cite{saltelli_variance_2010} in forms amenable to local decomposition. The local total sensitivity separately partitions the global sensitivity along each independent variable. We then adopt experimental designs that are suitable for iterative extension. The iterative experimental design samples the space of independent variables proportionally to the local variance indices observed so far in the experiment.

\begin{table}
    \centering
    \begin{tabular}{ll}
        Symbol & Definition \\
        \hline
        $i \in \{1 \ldots m\}$ & independent-variable index and dimension \\
        $j \in \{1 \ldots n\}$ & dependent-variable index and dimension \\
        $k \in \{1 \ldots N\}$ & observation index and dimension \\
        $\mathbb{E}_Z$ & expectation over random variable(s) $Z \in \{ X_i, \mathbf{X}_{\sim i} \}$ \\
        $\mathbb{V}_Z$ & variance over random variable(s) $Z \in \{ X_i, \mathbf{X}_{\sim i} \}$ \\
        $X_i \in [0, 1]$ & independent variable for dimension $i$ \\
        $\mathbf{X}_{\sim i}$ & independent variables except for dimension $i$ \\
        $Y_j$ & dependent variable for dimension $j$ \\
        $\mathbf{A}$, $\mathbf{B}$ & $N \times k$ design matrices \\
        $\mathbf{A}_{\mathbf{B}_i}$ & design matrix $\mathbf{A}$ with its $i$th column replaced \\ & by the $i$th column of $\mathbf{B}$ \\
        $x_{\mathbf{Z}_i}^{(k)} \in [0, 1]$ & $i$th independent variable for the $k$th observation \\ & in the design matrix $\mathbf{Z} \in \{ \mathbf{A}, \mathbf{B} \}$ \\
        $y_{\mathbf{Z},j}^{(k)}$ & $j$th dependent variable for the $k$th observation \\ & using the design matrix $\mathbf{Z} \in \{ \mathbf{A}, \mathbf{B}, \mathbf{A}_{\mathbf{B}_i} \}$ \\
        $\mathbbm{1}_c$ & indicator function: $1$ if the condition $c$ holds, \\ & but $0$ otherwise \\
        $u \mathrel{\hat=} v$ & $v$ is an estimate for $u$ \\
    \end{tabular}
    \caption{Notation used in this paper, inspired by and generally consistent with \cite{saltelli_variance_2010}.}
    \label{tab:notation}
\end{table}


\subsection{Global Sensitivity}

We start from standard definitions \cite{saltelli_variance_2010} that use conditional expectations and variances to express the first-order and total sensitivity indices, respectively:
\begin{align}
    S_{i,j} & = \frac{\mathbb{V}_{X_i} \left[ \mathbb{E}_{\mathbf{X}_{\sim i}} \left[ Y_j \middle| X_i \right] \right]}{\mathbb{V}_\mathbf{X} \left[ Y_j \right]} \label{eq:satellis}
    \\
    T_{i,j} & = \frac{\mathbb{E}_{\mathbf{X}_{\sim i}} \left[ \mathbb{V}_{X_i} \left[ Y_j \middle| \mathbf{X}_{\sim i} \right] \right]}{\mathbb{V}_\mathbf{X} \left[ Y_j \right]} \label{eq:saltellit}
\end{align}
The first-order index $S_{i,j}$ quantifies the influence of $X_i$ solely upon the variance of $Y_j$ whereas the total index $T_{i,j}$ quantifies the influence of $X_i$ in combination with other independent variables upon the variance of $Y_j$. The two indices arise from decomposing the variance $Y_j$ and obey a variety of summation relationships and bounds---see [cite]\todo{Cite Saltelli books and review papers.} for detailed discussion.

Practically estimating Eqs.~(\ref{eq:satellis}) and~(\ref{eq:saltellit}) may be problematic because of computational complexity, finite sample size, and numerical issues. Careful design of experiments alleviates some of these difficulties. An efficient design employs two $N \times k$ design matrices, $\mathbf{A}$ and $\mathbf{B}$, that are hybridized into matrices $\mathbf{A}_{\mathbf{B}_i}$ that are constructed by replacing the $i$th column of $\mathbf{A}$ with the $i$th column of $\mathbf{B}$. The number of rows $N$ determines the size of the experiment, which requires evaluating $\mathbf{Y}\left(\mathbf{X}\right)$ for each row of $\mathbf{A}$, $\mathbf{B}$, and $\mathbf{A}_{\mathbf{B}_i}$: thus $N (k + 2)$ observations must be made. The column-swapping construction facilitates efficient evaluation of the conditional expectations and variances by controlling which independent variables are varied and by organizing the computation.

The best-practice recommendations of Saltelli~\cite{saltelli_variance_2010} are to use the formulae in Jansen~\cite{jansen_analysis_1999} when computing the variances of conditional expectations and the expectations of conditional variances:
\begin{align}
    \mathbb{V}_\mathbf{X} \left[ Y_j \right] - \mathbb{V}_{X_i} \left[ \mathbb{E}_{\mathbf{X}_{\sim i}} \left[ Y_j \middle| X_i \right] \right] & \mathrel{\hat=} \frac{1}{2 N} \sum_{k=1}^N \left| y_{\mathbf{B},j}^{(k)} - y_{\mathbf{A}_{\mathbf{B}_i},j}^{(k)} \right|^2 \label{eq:jansens}
    \\
    \mathbb{E}_{\mathbf{X}_{\sim i}} \left[ \mathbb{V}_{X_i} \left[ Y_j \middle| \mathbf{X}_{\sim i} \right] \right] & \mathrel{\hat=} \frac{1}{2 N} \sum_{k=1}^N \left| y_{\mathbf{A},j}^{(k)} - y_{\mathbf{A}_{\mathbf{B}_i},j}^{(k)} \right|^2 \label{eq:jansent}
\end{align}
Here the $y_{\mathbf{A},j}^{(k)}$, $y_{\mathbf{B},j}^{(k)}$, and $y_{\mathbf{A}_{\mathbf{B}_i},j}^{(k)}$ are the observation of $Y_j$ for the $k$th row of the design matrix $\mathbf{A}$, $\mathbf{B}$, and $\mathbf{A}_{\mathbf{B}_i}$, respectively. We supplement these formulae with the analogous formula for computing the overall variance:
\begin{equation}
    \mathbb{V}_\mathbf{X} \left[ Y_j \right] \mathrel{\hat=} \frac{1}{2 N} \sum_{k=1}^N \left| y_{\mathbf{A},j}^{(k)} - y_{\mathbf{B},j}^{(k)} \right|^2 \label{eq:jansenv}
\end{equation}

Combining Eqs.~(\ref{eq:jansens}) through (\ref{eq:jansenv}), we adopt the following estimators for first-order and total global sensitivities and variance:
\begin{align}
    \hat{S}_{i,j} = 1 - & \frac{\sum_{k=1}^N \left| y_{\mathbf{B},j}^{(k)} - y_{\mathbf{A}_{\mathbf{B}_i},j}^{(k)} \right|^2}{\sum_{k=1}^N \left| y_{\mathbf{A},j}^{(k)} - y_{\mathbf{B},j}^{(k)} \right|^2} \label{eq:ours}
    \\
    \hat{T}_{i,j} = & \frac{\sum_{k=1}^N \left| y_{\mathbf{A},j}^{(k)} - y_{\mathbf{A}_{\mathbf{B}_i},j}^{(k)} \right|^2}{\sum_{k=1}^N \left| y_{\mathbf{A},j}^{(k)} - y_{\mathbf{B},j}^{(k)} \right|^2} \label{eq:ourt}
    \\
    \hat{V}_{j} = & \frac{1}{2 N} \sum_{k=1}^N \left| y_{\mathbf{A},j}^{(k)} - y_{\mathbf{B},j}^{(k)} \right|^2 \label{eq:ourv}
\end{align}


\subsection{Experimental Design}

Although any of a number of design methods could be used for creating the $\mathbf{A}$ and $\mathbf{B}$ matrices, quasi-random sequences are well suited for computing the sensitivity indices and Ref.~\cite{saltelli_variance_2010} recommends using Sobol' sequences because of their low discrepancy properties. In this procedure, the $\mathbf{A}$ and $\mathbf{B}$ matrices are placed side-by-side to form an $N \times 2 k$ matrix where the $N$ rows are consecutive points in the $2 k$-dimensional Sobol' sequence. An experiment can very simply be enlarged just by appending additional rows of the Sobol' sequence, so this design procedure is well suited for iteratively extending a sensitivity analysis.

Without loss of generality, we require that the domain of each independent variable be $[0, 1]$. This is convenient because the range of the Sobol' sequence is also $[0, 1]$ in each dimension.


\subsection{Local Sensitivity}

In developing a local version of variance-based sensitivity, one would like to attribute the variance in output to a specific value or interval of the independent variables. Equation~(\ref{eq:ourt}), for example, involves evaluating $Y_j$ at the points $x_{\mathbf{A}_i}^{(k)}$ and $x_{\mathbf{B}_i}^{(k)}$ to compute the contribution $\left| y_{\mathbf{A},j}^{(k)} - y_{\mathbf{A}_{\mathbf{B}_i},j}^{(k)} \right| ^2$ to the global total sensitivity. For lack of a more specific way to assign this contribution to a particular value of the independent variable, we can simply spread this contribution uniformly between $x_{\mathbf{A}_i}^{(k)}$ and $x_{\mathbf{B}_i}^{(k)}$. Thus we define $t_{i,j}^{(\alpha,\epsilon)}(x_i)$ to be the local sensitivity of the output $y_j$ upon the input $x_i$:
\begin{equation}
    t_{i,j}^{(\alpha,\epsilon)}(x) = \mathlarger{\mathlarger{\sum}}_{k=1}^N \left| y_{\mathbf{A},j}^{(k)} - y_{\mathbf{A}_{\mathbf{B}_i},j}^{(k)} \right| ^ \alpha  \cdot \frac{\mathbbm{1}_{\min \left\{ x_{\mathbf{A}_i}^{(k)} , x_{\mathbf{B}_i}^{(k)} \right\} - \frac{\epsilon}{2} \leq x \leq \max \left\{ x_{\mathbf{A}_i}^{(k)} , x_{\mathbf{B}_i}^{(k)} \right\} + \frac{\epsilon}{2}}}{\left| x_{\mathbf{A}_i}^{(k)} - x_{\mathbf{B}_i}^{(k)} + \epsilon \right|} \label{eq:tx}
\end{equation}
Here we have introduced $\epsilon > 0$ to guard against division by zero, which could occur if the two $x_i$ coincide, and we have replaced the exponent $2$ by the general parameter $\alpha$. The parameter $\alpha$ amplifies the difference between the dependent variables, with $\alpha = 2$ corresponding to computing the variance. The parameter $\epsilon$ slightly widens the interval over which the contribution to the variance is spread. It is also useful to define a cumulative version of the local sensitivity:
\begin{equation}
    T_{i,j}^{(\alpha,\epsilon)}(x) = \frac{1}{2 N \hat{V}_j} \int_{-\infty}^x d x^\prime \; t_{i,j}^{(\alpha,\epsilon)}(x^\prime) \label{eq:cumt}
\end{equation}
Note that we can recover the global total sensitivity index, Equation (\ref{eq:ourt}), from the limit of Equation (\ref{eq:cumt}):
\begin{equation*}
    \hat{T}_{i,j} = \lim_{x \rightarrow \infty} T_{i,j}^{(2,\epsilon)}(x)
\end{equation*}

In order to sample an independent variable proportionally to $t_{i,j}^{(\alpha,\epsilon)}(x_i)$, we need a normalized version of it, which we call the sensitivity density:
\begin{equation}
    \tau_{i,j}^{(\alpha,\epsilon)}(x) = \left. \frac{t_{i,j}^{(\alpha,\epsilon)}(x)}{t_{i,j}^{(0,\epsilon)}(x)} \middle/ \int_{-\infty}^{\infty} d x^\prime \; \frac{t_{i,j}^{(\alpha,\epsilon)}(x^\prime)}{t_{i,j}^{(0,\epsilon)}(x^\prime)} \right. \label{eq:taux}
\end{equation}
The factor $t_{i,j}^{(0,\epsilon)}(x)$ in the denominators removes the non-uniformity in the sampling of $x_i$. Finally, we can further summarize the local sensitivity by averaging it over the $n$ dependent variables; we call this the average sensitivity density:
\begin{equation}
    \overline{\tau}_i^{(\alpha,\epsilon)}(x) = \frac{1}{n} \sum_{j=1}^n \tau_{i,j}^{(\alpha,\epsilon)}(x) \label{eq:taubar}
\end{equation}
It also has a cumulative version:
\begin{equation}
    \overline{T}_i^{(\alpha,\epsilon)}(x) = \int_{-\infty}^x d x^\prime \; \overline{\tau}_i^{(\alpha,\epsilon)}(x^\prime) \label{eq:tbar}
\end{equation}

Unfortunately, the expression of the first-order sensitivity in Eq.~(\ref{eq:ours}) as the difference of two terms precludes its analogous decomposition into a local first-order sensitivity index. One could create a local first-order \textit{insensitivity} index based on $1 - \hat{S}_{i,j}$, but the density of insensitivity would not be practical for iteratively adapting experimental designs.

\subsection{Adaptively Iterating Sensitivity Analysis}

We are now in a position to specify an algorithm for iteratively adapting the experimental design for a sensitivity analysis as that analysis proceeds. Let $M$ be the batch size for the iterations.
\begin{enumerate}
    \item Evaluate $y_{\mathbf{A},j}^{(k)}$, $y_{\mathbf{B},j}^{(k)}$, and $y_{\mathbf{A}_{\mathbf{B}_i},j}^{(k)}$ for the first $M$ rows of the design matrices $\mathbf{A}$, $\mathbf{B}$, and $\mathbf{A}_{\mathbf{B}_i}$.
    \item Compute the cumulative average sensitivity densities $\overline{T}_i^{(\alpha,\epsilon)}(x)$ for each independent variable.
    \item Append $M$ rows to the design matrices by taking the next $M$ points in the Sobol' sequence, but transform those points according to the average sensitivity density. Namely, the Sobol' point $s_\ell$ is mapped to ${\overline{T}_i^{(\alpha,\epsilon)}}^{-1}(s_\ell)$, where $i = 1 + (\ell - 1 \mod m)$, since $\ell \in \{ 1 \ldots 2 m \}$ because the $\mathbf{A}$ and $\mathbf{B}$ matrices are placed side-by-side in the Sobol' space.
    \item Evaluate $y_{\mathbf{A},j}^{(k)}$, $y_{\mathbf{B},j}^{(k)}$, and $y_{\mathbf{A}_{\mathbf{B}_i},j}^{(k)}$ for these $M$ new rows of the design matrix.
    \item Proceed to step \#2 above.
\end{enumerate}
For computational efficiency, the $\hat{V}_j$ and $T_{i,j}^{(\alpha,\epsilon)}(x)$ can be updated during each iteration instead of being computed from scratch. The batch size can be $M = 1$ if an online algorithm is desired. The parameter $\epsilon$ should be set to a small positive value. The parameter $\alpha$ allows one to make the sampling completely non-adapative ($\alpha = 0$) or highly adaptive (large positive $\alpha$): setting $\alpha$ too high may result in missing sensitivities in the input space if the batch size is too small, since the algorithm may home in on the first observed region of sensitivity, thus missing other areas of high sensitivity.


\subsection{Functional PCA}

\todo{Hybridize the method with functional PCA.}


\section{Results}

We evaluate the adaptive iterative approach to sensitivity analysis first using a toy model with a manageably small number of dimensions, and then with a complex simulation.


\subsection{Toy Model}

The Appendix~\ref{sec:toy} describes a class of toy models that have strong nonlinearities, configurable dimensionality, and specifiable discontinuities in the dependent variables. For convenience in exploring and plotting results, we select three input dimensions ($m = 3$) and three output dimensions ($n = 3$). The model has a zeroth-order discontinuity in the output at $x_1 \doteq 0.59$, a first-order discontinuity in the output at $x_2 \doteq 0.95$, and a second-order discontinuity in the output at $x_3 \doteq 0.10$.

Table~\ref{tab:toyglobal} shows that the global sensitivities when $N = M = 1000$ exhibit a diversity of sensitive and insensitive first-order and total sensitivity indices. The cumulative sensitivities $T_{i,j}^{(2,\epsilon)}(x)$ in Figure~\ref{fig:toy-tx} approach the global sensitivities $\hat{T}_{i,j}$ of Table~\ref{tab:toyglobal} as $x_i \rightarrow 1$. One can also see mild nonlinearities near the $x_1 \doteq 0.59$, and $x_3 \doteq 0.10$ discontinuities of the model. The local sensitivity density in Figure~\ref{fig:toy-tau} highlights the stronger contributions to the local variance near the model's zeroth- and first-order discontinuities ($x_1 \doteq 0.59$ and $x_2 \doteq 0.95$). Averaging over the output dimensions yields the average local sensitivity density in Figure~\ref{fig:toy-taubar}. Figure~\ref{fig:toy-density} shows how densely different regions are sampled in the first five batches of $M = 10$ observations when the sensitivity analysis proceeds adaptively:\ this illustrates how quickly the algorithm detects the areas of higher variance as it transitions from the initial uniform sampling to the sensitivity-emphasizing sampling. Furthermore, Figure~\ref{fig:toy-sample} displays two-dimensional projections of actual sampling for a batch size of $M = 10$, with 100 batches so $N = 1000$: one can see that the algorithm explores the whole space while concentrating on areas of high sensitivity.

\begin{figure}
    \centering
    \includegraphics[width=1.00\linewidth]{figures/toy-tx.png}
    \caption{Cumulative local sensitivity $T_{i,j}^{(2,\epsilon)}(x)$ for the toy model, with $\epsilon = 10^{-4}$ and $N = M = 1000$.}
    \label{fig:toy-tx}
\end{figure}

\begin{table}
\centering
\begin{tabular}{l|c|c|c}
$\hat{S}_{i,j}$ & $x_1$ & $x_2$ & $x_3$ \\
\hline
$y_1$ & 0.01 & 0.00 & 0.98 \\
$y_2$ & 0.62 & 0.00 & 0.06 \\
$y_3$ & 0.13 & 0.00 & 0.59 \\
\end{tabular}
\quad
\begin{tabular}{l|c|c|c}
$\hat{T}_{i,j}$ & $x_1$ & $x_2$ & $x_3$ \\
\hline
$y_1$ & 0.01 & 0.00 & 0.99 \\
$y_2$ & 0.94 & 0.00 & 0.38 \\
$y_3$ & 0.41 & 0.00 & 0.87 \\
\end{tabular}
\caption{Global first-order sensitivity (left) and total sensitivities (right) results for the toy model, on a scale from zero (insensitive) to one (sensitive), with one large batch of observations $N = M = 1000$.}
\label{tab:toyglobal}
\end{table}

\begin{figure}
    \centering
    \includegraphics[width=1.00\linewidth]{figures/toy-tau.png}
    \caption{Sensitivity density $\tau_{i,j}^{(2,\epsilon)}(x)$ for the toy model, with $\epsilon = 10^{-4}$ and $N = M = 1000$. (Note that the density is smoothed using a general additive model.)}
    \label{fig:toy-tau}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1.00\linewidth]{figures/toy-taubar.png}
    \caption{Averaged sensitivity density $\overline{\tau}_i^{(2,\epsilon)}(x)$ for the toy model, with $\epsilon = 10^{-4}$ and $N = M = 1000$. The three dashed orange lines mark locations of the toy model's discontinuities with respect to its three independent variables: the algorithm successfully detects the zeroth- and first-order discontinuities in $x_1$ and $x_2$, respectively, but not the second-order discontinuity in $x_3$. (Note that the density is smoothed using a general additive model.)}
    \label{fig:toy-taubar}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.300\linewidth]{figures/toy-density1.png}
    \includegraphics[width=0.300\linewidth]{figures/toy-density2.png}
    \includegraphics[width=0.300\linewidth]{figures/toy-density3.png}
    \includegraphics[width=0.075\linewidth,clip=true,trim={1.5in 0 2.15 2.15}]{figures/toy-density.png}
    \caption{Density of sampling in the first five batches of $M = 10$ observations in adaptive sampling according to $\overline{\tau}_i^{(2,\epsilon)}(x)$ for the toy model, with $\epsilon = 10^{-4}$, starting with a first batch that uniformly samples. The three dashed orange lines mark locations of the toy model's discontinuities with respect to its three independent variables: the algorithm successfully detects the zeroth- and first-order discontinuities in $x_1$ and $x_2$, respectively, but not the second-order discontinuity in $x_3$. (Note that the density is smoothed using a general additive model.)}
    \label{fig:toy-density}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.300\linewidth]{figures/toy-sample12.png}
    \includegraphics[width=0.300\linewidth]{figures/toy-sample23.png}
    \includegraphics[width=0.300\linewidth]{figures/toy-sample31.png}
    \includegraphics[width=0.075\linewidth,clip=true,trim={1.5in 0 2.15 2.15}]{figures/toy-sample.png}
    \caption{Two-dimensional projections of adaptive sampling according to $\overline{\tau}_i^{(2,\epsilon)}(x)$ for the toy model, with $\epsilon = 10^{-4}$ and for 100 sequential batches of $M = 10$ observations, so $N = 1000$. The solid black lines mark locations of the toy model's discontinuities with respect to its three independent variables: the algorithm successfully detects the zeroth- and first-order discontinuities in $x_1$ and $x_2$, respectively, but not the second-order discontinuity in $x_3$.}
    \label{fig:toy-sample}
\end{figure}


\subsection{Bioproduct Transition Dynamics Model}

Figure~\ref{fig:btd-t}
Figure~\ref{fig:btd-tau}
Figure~\ref{fig:btd-taubar1}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/btd-t.png}
    \caption{Cumulative local sensitivity $T_{i,j}^{(2,\epsilon)}(x)$ for the BTD model, with $j =$ ``demo plant construction'' and $N = M = 2500$. The black curve is the computed sensitivity, the 25 blue curves are sensitivities obtained by resampling the observations, and the orange curve is the fit from a general additive model for the blue curves.}
    \label{fig:btd-t}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/btd-tau.png}
    \caption{Sensitivity density $\tau_{i,j}^{(2,\epsilon)}(x)$ for the BTD model, with $j =$ ``demo plant construction'' and $N = M = 2500$. (Note that the density is smoothed using LOESS, with dark gray bands for the confidence interval of the smoothed curve.)}
    \label{fig:btd-tau}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/btd-taubar1.png}
    \caption{Average sensitivity density $\overline{\tau}_i^{(2,\epsilon)}(x)$ for the BTD model, for the first batch of $M = 10$ observations (blue with dark gray) and for the full set of $N = 2500$ observations (orange). (Note that the density is smoothed using LOESS, with dark gray bands for the confidence interval of the smoothed curve.)}
    \label{fig:btd-taubar1}
\end{figure}


\section{Conclusion}

\todo{PCA could be applied on the input space, too, or tensor PCA (maybe functional tensor PCA) could be employed.}


\section*{Acknowledgements}

This work was authored in part by the National Renewable Energy Laboratory, operated by Alliance for Sustainable Energy, LLC, for the U.S. Department of Energy (DOE) under Contract No. DE-AC36-08GO28308. Funding provided by the Department of Energy Office of Energy Efficiency and Renewable Energy Bioenergy Technologies Office. The views expressed in the article do not necessarily represent the views of the DOE or the U.S.\ Government. The U.S.\ Government retains and the publisher, by accepting the article for publication, acknowledges that the U.S.\ Government retains a nonexclusive, paid-up, irrevocable, worldwide license to publish or reproduce the published form of this work, or allow others to do so, for U.S. Government purposes.


%%%% FIXME: Comment out the following command before submitting.
\nocite{*}


\printbibliography


\appendix


\section{Toy Model}
\label{sec:toy}

This toy model consists of a nonlinear ordinary differential equation for $y_j$ as a function of $x_i$, with $i \in \{ 1 \ldots m \}$ and $j \in \{ 1 \ldots n \}$:
\begin{equation*}
    \frac{dy}{dt} = \kappa \cdot y \ z \ \left( 1 - \sigma \cdot y \  z \right) ,
\end{equation*}
where
\begin{equation*}
    z_j = \sum_{i=1}^m \mathbbm{1}_{j \in L_i} \mathbbm{1}_{x_i \ge \xi_i} \zeta_i \left( x_i - \xi_i \right)^{\delta_i} .
\end{equation*}
The parameters $\xi_i$ define locations of discontinuities and the parameters $\delta_i$ specify the degree of those discontinuities. The parameters $L_i$ affect the mixing of the input variables into the output, as do the matrices $\kappa$ and $\sigma$. The model was designed to exhibit nontrivial and nonlinear behavior with tunable discontinuous behavior. It can be used as a timeseries generator or just evaluated at a specified final time.

The particular instantiation of the toy model studied in this paper uses the following parameter values:
\begin{align*}
    m & = 3
    \\
    n & = 3
    \\
    t & \in [0, 10]
    \\
    L & = \begin{bmatrix} \left\{ 2, 3 \right\} & \left\{ 1, 2 \right\} & \left\{ 1, 2, 3 \right\} \end{bmatrix}
    \\
    \delta & = \begin{bmatrix} 0 & 1 & 2 \end{bmatrix}
    \\
    \xi & = \begin{bmatrix} 0.5933 & 0.9485 & 0.1030 \end{bmatrix}
    \\
    \zeta & = \begin{bmatrix} 0.8788 & 0.2668 & 0.6661 \end{bmatrix}
    \\
    y(0) & = \begin{bmatrix} -0.1900 & 0.5145 & 0.4094 \end{bmatrix}
    \\
    \kappa & = \begin{bmatrix} 0.7054 & 0.2921 & 0.7361 \\ -0.1151 & 0.5206 & -0.0707 \\ 0.3475 & -0.0579 & -0.2229 \end{bmatrix}
    \\
    \sigma & = \begin{bmatrix} 0.0294 & 0.1668 & 0.5788 \\ 0.1046 & 0.1705 & 0.2749 \\ -0.1258 & -0.0712 & 0.7372 \end{bmatrix}
\end{align*}

The following R code implements this toy model:

\begin{small}
\begin{verbatim}
# Create a multivariate function with specified properties:
#   tmax: maximum time
#   multiplicities: number of correlations each parameter has
#   degrees: polynomial degree of each parameter
#   dimension: the dimension of the output
#   returns a multivariate function of the vector of parameters and time
makeGenerator <- function(multiplicities, degrees, dimension) {
    single <- function(degree) {
      x0 <- runif(1)
      z0 <- runif(1)
      print(paste("Critical point at x = ", x0, sep = ""))
      function(x) {
          if (x < x0)
              0
          else
              z0 * (x - x0)^degree
      }
    }
    locations <- lapply(
        multiplicities,
        function(m) sample(1:dimension, m)
    )
    functions <- lapply(degrees, single)
    start <- runif(dimension, -0.25, 0.75)
    coefs <- matrix(
        runif(dimension^2, -0.25, 0.75),
        dimension,
        dimension
    )    
    shift <- matrix(
        runif(dimension^2, -0.25, 0.75),
        dimension,
        dimension
    )
    function(x, ts) {
        z <- rep(0, dimension)
        for (i in 1:length(locations))
            for (j in locations[[i]])
                z[j] <- z[j] + functions[[i]](x[i])
        ode(start, ts, function(t, y, params) {list((coefs %*% y) * z *
            (1 - ((shift %*% y) * z)))})
    }
}
\end{verbatim}
\end{small}

Here is a simple example of the use of this function:
\begin{small}
\begin{verbatim}
# Use reproducible random numbers.
RNGkind("Mersenne-Twister", "Inversion", "Rejection")
set.seed(46)

# Instantiate the model.
f <- makeGenerator(c(2, 2, 3), c(0, 1, 2), 3)
#   "Critical point at x = 0.593385165324435"
#   "Critical point at x = 0.948547213338315"
#   "Critical point at x = 0.102978735696524"

# Evaluate it at x = (0.1, 0.2, 0.3) for t = 0, 5, 10.
f(c(0.1, 0.2, 0.3), c(0, 5, 10))
# time    1          2          3
# 0     -0.1900320  0.5144967  0.4093612
# 5     -0.1478757  0.5489932  0.3864914
#10     -0.1024813  0.5854096  0.3659173
\end{verbatim}
\end{small}


\end{document}
