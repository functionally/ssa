@article{saltelli_variance_2010,
	title = {Variance based sensitivity analysis of model output. {Design} and estimator for the total sensitivity index},
	volume = {181},
	issn = {0010-4655},
%   url = {http://www.sciencedirect.com/science/article/pii/S0010465509003087},
	doi = {10.1016/j.cpc.2009.09.018},
	abstract = {Variance based methods have assessed themselves as versatile and effective among the various available techniques for sensitivity analysis of model output. Practitioners can in principle describe the sensitivity pattern of a model Y=f(X1,X2,…,Xk) with k uncertain input factors via a full decomposition of the variance V of Y into terms depending on the factors and their interactions. More often practitioners are satisfied with computing just k first order effects and k total effects, the latter describing synthetically interactions among input factors. In sensitivity analysis a key concern is the computational cost of the analysis, defined in terms of number of evaluations of f(X1,X2,…,Xk) needed to complete the analysis, as f(X1,X2,…,Xk) is often in the form of a numerical model which may take long processing time. While the computational cost is relatively cheap and weakly dependent on k for estimating first order effects, it remains expensive and strictly k-dependent for total effect indices. In the present note we compare existing and new practices for this index and offer recommendations on which to use.},
	language = {en},
	number = {2},
	urldate = {2020-08-11},
	journal = {Computer Physics Communications},
	author = {Saltelli, Andrea and Annoni, Paola and Azzini, Ivano and Campolongo, Francesca and Ratto, Marco and Tarantola, Stefano},
	month = feb,
	year = {2010},
	pages = {259--270},
	file = {ScienceDirect Snapshot:/Users/bbush/Zotero/storage/BYDKQXR5/S0010465509003087.html:text/html;ScienceDirect Full Text PDF:/Users/bbush/Zotero/storage/XRT5H87I/Saltelli et al. - 2010 - Variance based sensitivity analysis of model outpu.pdf:application/pdf}
}


@article{jansen_analysis_1999,
	title = {Analysis of variance designs for model output},
	volume = {117},
	issn = {0010-4655},
%   url = {http://www.sciencedirect.com/science/article/pii/S0010465598001544},
	doi = {10.1016/S0010-4655(98)00154-4},
	abstract = {A scalar model output Y is assumed to depend deterministically on a set of stochastically independent input vectors of different dimensions. The composition of the variance of Y is considered; variance components of particular relevance for uncertainty analysis are identified. Several analysis of variance designs for estimation of these variance components are discussed. Classical normal-model theory can suggest optimal designs. The designs can be implemented with various sampling methods: ordinary random sampling, latin hypercube sampling and scrambled quasi-random sampling. Some combinations of design and sampling method are compared in two small-scale numerical experiments.},
	language = {en},
	number = {1},
	urldate = {2020-09-20},
	journal = {Computer Physics Communications},
	author = {Jansen, Michiel J. W.},
	month = mar,
	year = {1999},
	keywords = {Experimental design, Latin hypercube sampling, regression-free, Scrambled quasi-random sampling, uncertainty analysis, Variance-based},
	pages = {35--43},
	file = {ScienceDirect Snapshot:/Users/bbush/Zotero/storage/S3GU32HR/S0010465598001544.html:text/html;Jansen - 1999 - Analysis of variance designs for model output.pdf:/Users/bbush/Zotero/storage/XH7YTKAX/Jansen - 1999 - Analysis of variance designs for model output.pdf:application/pdf}
}


@article{jadun_application_2017,
	title = {Application of a variance-based sensitivity analysis method to the {Biomass} {Scenario} {Learning} {Model}},
	volume = {33},
	issn = {0883-7066},
	url = {https://www.osti.gov/pages/biblio/1459456-application-variance-based-sensitivity-analysis-method-biomass-scenario-learning-model},
	doi = {10.1002/sdr.1594},
	abstract = {The U.S. Department of Energy's Office of Scientific and Technical Information},
	language = {English},
	number = {3-4},
	urldate = {2018-12-21},
	journal = {System Dynamics Review},
	author = {Jadun, Paige and Inman, Daniel J. and Bush, Brian W. (ORCID:0000000328647028) and Vimmerstedt, Laura J. and Peterson, Steve},
	month = jul,
	year = {2017}
}


@article{inman_application_2018,
	title = {Application of {Variance}-{Based} {Sensitivity} {Analysis} to a {Large} {System} {Dynamics} {Model}},
	url = {https://www.osti.gov/biblio/1468525-application-variance-based-sensitivity-analysis-large-system-dynamics-model},
	abstract = {The U.S. Department of Energy's Office of Scientific and Technical Information},
	language = {English},
	urldate = {2018-12-21},
	journal = {Cornell University Library Preprint Archive},
	author = {Inman, Daniel J. and Vimmerstedt, Laura J. and Bush, Brian W. (ORCID:0000000328647028) and Stright, Dana K. and Peterson, Steve},
	month = mar,
	year = {2018}
}

@article{noauthor_sensitivity_1995,
	title = {Sensitivity analysis of model output. {Performance} of the iterated fractional factorial design method},
	volume = {20},
	issn = {0167-9473},
	url = {https://www.sciencedirect.com/science/article/pii/016794739592843M},
	doi = {10.1016/0167-9473(95)92843-M},
	abstract = {The present article is a sequel to an earlier study in this journal (Saltelli et al., 1993) where two new sensitivity analysis techniques were present…},
	language = {en},
	number = {4},
	urldate = {2020-04-07},
	journal = {Computational Statistics \& Data Analysis},
	month = oct,
	year = {1995},
	note = {Publisher: North-Holland},
	pages = {387--407},
	file = {Snapshot:/Users/bbush/Zotero/storage/LNPBB8GH/016794739592843M.html:text/html;1995 - Sensitivity analysis of model output. Performance .pdf:/Users/bbush/Zotero/storage/86WJ56YE/1995 - Sensitivity analysis of model output. Performance .pdf:application/pdf}
}

@article{tian_bootstrap_2014,
	title = {Bootstrap techniques for sensitivity analysis and model selection in building thermal performance analysis},
	volume = {135},
	issn = {0306-2619},
	url = {http://www.sciencedirect.com/science/article/pii/S0306261914009337},
	doi = {10.1016/j.apenergy.2014.08.110},
	abstract = {In regression analysis, there are two main aims: interpretation and prediction, which can be also applied in building performance analysis. Interpretation is used to understand the relationship between input parameters and building energy performance (also called sensitivity analysis), whereas prediction is used to create a reliable energy model to estimate building energy consumption. This article explores the implementation of a distribution-free bootstrap method for these two purposes. The bootstrap is a resampling method that enables assessment of the accuracy of an estimator by random sampling with replacement from an original dataset. An office building is used as a case study to demonstrate the application of this method in assessing building thermal performance. The results indicate that the probabilistic sensitivity analysis incorporating the bootstrap approach provides valuable insights into the variations in sensitivity indicators, which are not available from typical deterministic sensitivity analysis. The single point values from deterministic methods may lead to misleading prioritization of energy saving measures because they do not provide the distributions of sensitivity indicators. Information on prediction errors obtained from the bootstrap method can facilitate the selection of an appropriate building energy metamodel to more accurately predict the energy consumption of buildings, compared with the traditional one-time data splitting method (also called holdout cross-validation method), which partitions the data into a training set and a test set.},
	language = {en},
	urldate = {2020-04-18},
	journal = {Applied Energy},
	author = {Tian, Wei and Song, Jitian and Li, Zhanyong and de Wilde, Pieter},
	month = dec,
	year = {2014},
	keywords = {Sensitivity analysis, Model selection, Bootstrap method, Building thermal performance},
	pages = {320--328},
	file = {ScienceDirect Snapshot:/Users/bbush/Zotero/storage/DVLPD4VR/S0306261914009337.html:text/html;ScienceDirect Full Text PDF:/Users/bbush/Zotero/storage/46BUV4A4/Tian et al. - 2014 - Bootstrap techniques for sensitivity analysis and .pdf:application/pdf}
}


@article{piano_new_2019,
	title = {A new sample-based algorithms to compute the total sensitivity index},
	url = {http://arxiv.org/abs/1703.05799},
	abstract = {Variance-based sensitivity indices have established themselves as a reference among practitioners of sensitivity analysis of model output. It is not unusual to consider a variance-based sensitivity analysis as informative if it produces at least the first order sensitivity indices S\_j and the so-called total-effect sensitivity indices T\_j for all the uncertain factors of the mathematical model under analysis. Computational economy is critical in sensitivity analysis. It depends mostly upon the number of model evaluations needed to obtain stable values of the estimates. While efficient estimation procedures independent from the number of factors under analysis are available for the first order indices, this is less the case for the total sensitivity indices. When estimating T\_j, one can either use a sample-based approach, whose computational cost depends fromon the number of factors, or approaches based on meta-modelling/emulators, e.g. based on Gaussian processes. The present work focuses on sample-based estimation procedures for T\_j and tries different avenues to achieve an algorithmic improvement over the designs proposed in the existing best practices. We conclude that some proposed sample-based improvements found in the literature do not work as claimed, and that improving on the existing best practice is indeed fraught with difficulties. We motivate our conclusions introducing the concepts of explorativity and efficiency of the design.},
	urldate = {2020-04-19},
	journal = {arXiv:1703.05799 [stat]},
	author = {Piano, Samuele Lo and Ferretti, Federico and Puy, Arnald and Albrecht, Daniel and Tarantola, Stefano and Saltelli, Andrea},
	month = may,
	year = {2019},
	note = {arXiv: 1703.05799},
	keywords = {Statistics - Applications, G.3, 00A71, G.4},
	file = {arXiv.org Snapshot:/Users/bbush/Zotero/storage/DKTFYUP6/1703.html:text/html;arXiv Fulltext PDF:/Users/bbush/Zotero/storage/7VEU2QQ7/Piano et al. - 2019 - A new sample-based algorithms to compute the total.pdf:application/pdf}
}


@article{saltelli_making_2002,
	title = {Making best use of model evaluations to compute sensitivity indices},
	volume = {145},
	issn = {00104655},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0010465502002801},
	doi = {10.1016/S0010-4655(02)00280-1},
	language = {en},
	number = {2},
	urldate = {2020-04-19},
	journal = {Computer Physics Communications},
	author = {Saltelli, Andrea},
	month = may,
	year = {2002},
	pages = {280--297},
	file = {Saltelli - 2002 - Making best use of model evaluations to compute se.pdf:/Users/bbush/Zotero/storage/2LQX7JSM/Saltelli - 2002 - Making best use of model evaluations to compute se.pdf:application/pdf}
}

@article{borgonovo_sensitivity_2016,
	title = {Sensitivity analysis: {A} review of recent advances},
	volume = {248},
	issn = {0377-2217},
	shorttitle = {Sensitivity analysis},
	url = {http://www.sciencedirect.com/science/article/pii/S0377221715005469},
	doi = {10.1016/j.ejor.2015.06.032},
	abstract = {The solution of several operations research problems requires the creation of a quantitative model. Sensitivity analysis is a crucial step in the model building and result communication process. Through sensitivity analysis we gain essential insights on model behavior, on its structure and on its response to changes in the model inputs. Several interrogations are possible and several sensitivity analysis methods have been developed, giving rise to a vast and growing literature. We present an overview of available methods, structuring them into local and global methods. For local methods, we discuss Tornado diagrams, one way sensitivity functions, differentiation-based methods and scenario decomposition through finite change sensitivity indices, providing a unified view of the associated sensitivity measures. We then analyze global sensitivity methods, first discussing screening methods such as sequential bifurcation and the Morris method. We then address variance-based, moment-independent and value of information-based sensitivity methods. We discuss their formalization in a common rationale and present recent results that permit the estimation of global sensitivity measures by post-processing the sample generated by a traditional Monte Carlo simulation. We then investigate in detail the methodological issues concerning the crucial step of correctly interpreting the results of a sensitivity analysis. A classical example is worked out to illustrate some of the approaches.},
	language = {en},
	number = {3},
	urldate = {2020-04-19},
	journal = {European Journal of Operational Research},
	author = {Borgonovo, Emanuele and Plischke, Elmar},
	month = feb,
	year = {2016},
	keywords = {Simulation, Sensitivity analysis, Computer experiments},
	pages = {869--887},
	file = {ScienceDirect Snapshot:/Users/bbush/Zotero/storage/LSSYP6ZY/S0377221715005469.html:text/html;ScienceDirect Full Text PDF:/Users/bbush/Zotero/storage/9CKD5A8P/Borgonovo and Plischke - 2016 - Sensitivity analysis A review of recent advances.pdf:application/pdf}
}

@article{tian_review_2013,
	title = {A review of sensitivity analysis methods in building energy analysis},
	volume = {20},
	issn = {1364-0321},
	url = {http://www.sciencedirect.com/science/article/pii/S1364032112007101},
	doi = {10.1016/j.rser.2012.12.014},
	abstract = {Sensitivity analysis plays an important role in building energy analysis. It can be used to identify the key variables affecting building thermal performance from both energy simulation models and observational study. This paper is focused on the application of sensitivity analysis in the field of building performance analysis. First, the typical steps of implementation of sensitivity analysis in building analysis are described. A number of practical issues in applying sensitivity analysis are also discussed, such as the determination of input variations, the choice of building energy programs, how to reduce computational time for energy models. Second, the sensitivity analysis methods used in building performance analysis are reviewed. These methods can be categorized into local and global sensitivity analysis. The global methods can be further divided into four approaches: regression, screening-based, variance-based, and meta-model sensitivity analysis. Recent research has been concentrated on global methods because they can explore the whole input space and most of them allow the self-verification, i.e., how much variance of the model output (building energy consumption) has been explained by the method used in the analysis. Third, we discuss several important topics, which are often overlooked in the domain of building performance analysis. These topics include the application of sensitivity analysis in observational study, how to deal with correlated inputs, the computation of the variations of sensitivity index, and the software issues. Lastly, the practical guidance is given based on the advantages and disadvantaged of different sensitivity analysis methods in assessing building thermal performance. The recommendations for further research in the future are made to provide more robust analysis in assessing building energy performance.},
	language = {en},
	urldate = {2020-04-19},
	journal = {Renewable and Sustainable Energy Reviews},
	author = {Tian, Wei},
	month = apr,
	year = {2013},
	keywords = {Sensitivity analysis, Global method, Building energy, Correlated inputs, Variance-based method},
	pages = {411--419},
	file = {ScienceDirect Snapshot:/Users/bbush/Zotero/storage/4MBEVHPK/S1364032112007101.html:text/html;ScienceDirect Full Text PDF:/Users/bbush/Zotero/storage/NIT9CC8M/Tian - 2013 - A review of sensitivity analysis methods in buildi.pdf:application/pdf}
}

@article{li_new_2016,
	title = {A new kind of sensitivity index for multivariate output},
	volume = {147},
	issn = {0951-8320},
	url = {http://www.sciencedirect.com/science/article/pii/S0951832015003336},
	doi = {10.1016/j.ress.2015.11.006},
	abstract = {Mathematical and computational models with correlated multivariate output are commonly used for risk assessment and decision support in engineering. Traditional methods for sensitivity analysis of the model with scalar output fail to provide satisfactory results for this multivariate case. In this work, we introduce a new sensitivity index which looks at the influence of input uncertainty on the entire distribution of the multivariate output without reference to a specific moment of the output. The definition of the new index is based on the multivariate probability integral transformation (PIT), which can take into account both of the uncertainties and the correlations among multivariate output. The mathematical properties of the proposed sensitivity index are discussed and its differences with the sensitivity indices previously introduced in the literature are highlighted. Two numerical examples and a rotating shaft model of an aircraft wing are employed to illustrate the validity and potential benefits of the new sensitivity index.},
	language = {en},
	urldate = {2020-04-19},
	journal = {Reliability Engineering \& System Safety},
	author = {Li, Luyi and Lu, Zhenzhou and Wu, Danqing},
	month = mar,
	year = {2016},
	keywords = {Sensitivity analysis, Cumulative distribution function, Multivariate output, Multivariate probability integral transformation},
	pages = {123--131},
	file = {ScienceDirect Snapshot:/Users/bbush/Zotero/storage/BVKNFGSX/S0951832015003336.html:text/html;ScienceDirect Full Text PDF:/Users/bbush/Zotero/storage/5APQWFSW/Li et al. - 2016 - A new kind of sensitivity index for multivariate o.pdf:application/pdf}
}

@article{tissot_bias_2012,
	series = {{SAMO} 2010},
	title = {Bias correction for the estimation of sensitivity indices based on random balance designs},
	volume = {107},
	issn = {0951-8320},
	url = {http://www.sciencedirect.com/science/article/pii/S0951832012001159},
	doi = {10.1016/j.ress.2012.06.010},
	abstract = {This paper deals with the random balance design method (RBD) and its hybrid approach, RBD-FAST. Both these global sensitivity analysis methods originate from Fourier amplitude sensitivity test (FAST) and consequently face the main problems inherent to discrete harmonic analysis. We present here a general way to correct a bias which occurs when estimating sensitivity indices (SIs) of any order – except total SI of single factor or group of factors – by the random balance design method (RBD) and its hybrid version, RBD-FAST. In the RBD case, this positive bias has been recently identified in a paper by Xu and Gertner [1]. Following their work, we propose a bias correction method for first-order SIs estimates in RBD. We then extend the correction method to the SIs of any order in RBD-FAST. At last, we suggest an efficient strategy to estimate all the first- and second-order SIs using RBD-FAST.},
	language = {en},
	urldate = {2020-04-19},
	journal = {Reliability Engineering \& System Safety},
	author = {Tissot, Jean-Yves and Prieur, Clémentine},
	month = nov,
	year = {2012},
	keywords = {Sensitivity indices, Global sensitivity analysis, Bias correction, Random balance design, RBD-FAST},
	pages = {205--213},
	file = {ScienceDirect Snapshot:/Users/bbush/Zotero/storage/TZ5SYKV2/S0951832012001159.html:text/html;ScienceDirect Full Text PDF:/Users/bbush/Zotero/storage/REFJ4XUA/Tissot and Prieur - 2012 - Bias correction for the estimation of sensitivity .pdf:application/pdf}
}

@article{saint-geours_computing_2015,
	title = {Computing first-order sensitivity indices with contribution to the sample mean plot},
	volume = {85},
	issn = {0094-9655},
	url = {https://doi.org/10.1080/00949655.2014.932358},
	doi = {10.1080/00949655.2014.932358},
	abstract = {In this paper, we investigate the use of the contribution to the sample mean plot (CSM plot) as a graphical tool for sensitivity analysis (SA) of computational models. We first provide an exact formula that links, for each uncertain model input Xj, the CSM plot Cj(·) with the first-order variance-based sensitivity index Sj. We then build a new estimate for Sj using polynomial regression of the CSM plot. This estimation procedure allows the computation of Sj from given data, without any SA-specific design of experiment. Numerical results show that this new Sj estimate is efficient for large sample sizes, but that at small sample sizes it does not compare well with other Sj estimation techniques based on given data, such as the effective algorithm for computing global sensitivity indices method or metamodel-based approaches.},
	number = {7},
	urldate = {2020-04-19},
	journal = {Journal of Statistical Computation and Simulation},
	author = {Saint-Geours, N. and Tarantola, S. and Kopustinskas, V. and Bolado-Lavin, R.},
	month = may,
	year = {2015},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/00949655.2014.932358},
	keywords = {variance-based sensitivity analysis, Monte Carlo simulation, 49Q12, 65S05, contribution to the sample mean plot, importance measures, Ishigami function},
	pages = {1334--1357},
	file = {Snapshot:/Users/bbush/Zotero/storage/98IDJJQ7/00949655.2014.html:text/html}
}

@article{pianosi_sensitivity_2016,
	title = {Sensitivity analysis of environmental models: {A} systematic review with practical workflow},
	volume = {79},
	issn = {1364-8152},
	shorttitle = {Sensitivity analysis of environmental models},
	url = {http://www.sciencedirect.com/science/article/pii/S1364815216300287},
	doi = {10.1016/j.envsoft.2016.02.008},
	abstract = {Sensitivity Analysis (SA) investigates how the variation in the output of a numerical model can be attributed to variations of its input factors. SA is increasingly being used in environmental modelling for a variety of purposes, including uncertainty assessment, model calibration and diagnostic evaluation, dominant control analysis and robust decision-making. In this paper we review the SA literature with the goal of providing: (i) a comprehensive view of SA approaches also in relation to other methodologies for model identification and application; (ii) a systematic classification of the most commonly used SA methods; (iii) practical guidelines for the application of SA. The paper aims at delivering an introduction to SA for non-specialist readers, as well as practical advice with best practice examples from the literature; and at stimulating the discussion within the community of SA developers and users regarding the setting of good practices and on defining priorities for future research.},
	language = {en},
	urldate = {2020-04-19},
	journal = {Environmental Modelling \& Software},
	author = {Pianosi, Francesca and Beven, Keith and Freer, Jim and Hall, Jim W. and Rougier, Jonathan and Stephenson, David B. and Wagener, Thorsten},
	month = may,
	year = {2016},
	keywords = {Calibration, Evaluation, Robust decision-making, Sensitivity Analysis, Uncertainty Analysis},
	pages = {214--232},
	file = {ScienceDirect Snapshot:/Users/bbush/Zotero/storage/L5MM4SJF/S1364815216300287.html:text/html;ScienceDirect Full Text PDF:/Users/bbush/Zotero/storage/4LWQ3GPW/Pianosi et al. - 2016 - Sensitivity analysis of environmental models A sy.pdf:application/pdf}
}

@article{norton_introduction_2015,
	title = {An introduction to sensitivity assessment of simulation models},
	volume = {69},
	issn = {1364-8152},
	url = {http://www.sciencedirect.com/science/article/pii/S1364815215001085},
	doi = {10.1016/j.envsoft.2015.03.020},
	abstract = {In view of increasing application of sensitivity assessment (SA) to environmental simulation models, a relatively short, informal introduction to aims and methods of SA is given. Their variety, motivation and scope are illustrated by outlines of a broad selection of approaches. Methods based on derivatives, algebraic analysis, sparse sampling, variance decomposition, Fourier analysis and binary classification are included.},
	language = {en},
	urldate = {2020-04-19},
	journal = {Environmental Modelling \& Software},
	author = {Norton, John},
	month = jul,
	year = {2015},
	keywords = {Sensitivity analysis, Algebraic sensitivity analysis, Emulators, Overview, Sensitivity index, Simulation models},
	pages = {166--174},
	file = {ScienceDirect Snapshot:/Users/bbush/Zotero/storage/F8VSW2FN/S1364815215001085.html:text/html;ScienceDirect Full Text PDF:/Users/bbush/Zotero/storage/MWJNDSAM/Norton - 2015 - An introduction to sensitivity assessment of simul.pdf:application/pdf}
}

@article{spear_parameter_1994,
	title = {Parameter uncertainty and interaction in complex environmental models},
	volume = {30},
	copyright = {Copyright 1994 by the American Geophysical Union.},
	issn = {1944-7973},
	url = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/94WR01732},
	doi = {10.1029/94WR01732},
	abstract = {Recently developed models for the estimation of risks arising from the release of toxic chemicals from hazardous waste sites are inherently complex both structurally and parametrically. To better understand the impact of uncertainty and interaction in the high-dimensional parameter spaces of these models, the set of procedures termed regional sensitivity analysis has been extended and applied to the groundwater pathway of the MMSOILS model. The extension consists of a tree-structured density estimation technique which allows the characterization of complex interaction in that portion of the parameter space which gives rise to successful simulation. Results show that the parameter space can be partitioned into small, densely populated regions and relatively large, sparsely populated regions. From the high-density regions one can identify the important or controlling parameters as well as the interaction between parameters in different local areas of the space. This new tool can provide guidance in the analysis and interpretation of site-specific application of these complex models.},
	language = {en},
	number = {11},
	urldate = {2020-04-19},
	journal = {Water Resources Research},
	author = {Spear, Robert C. and Grieb, Thomas M. and Shang, Nong},
	year = {1994},
	note = {\_eprint: https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/94WR01732},
	pages = {3159--3169},
	file = {Snapshot:/Users/bbush/Zotero/storage/5E2JCKIM/94WR01732.html:text/html}
}

@article{kucherenko_estimation_2012,
	title = {Estimation of global sensitivity indices for models with dependent variables},
	volume = {183},
	issn = {0010-4655},
	url = {http://www.sciencedirect.com/science/article/pii/S0010465511004085},
	doi = {10.1016/j.cpc.2011.12.020},
	abstract = {A novel approach for estimation variance-based sensitivity indices for models with dependent variables is presented. Both the first order and total sensitivity indices are derived as generalizations of Sobolʼ sensitivity indices. Formulas and Monte Carlo numerical estimates similar to Sobolʼ formulas are derived. A copula-based approach is proposed for sampling from arbitrary multivariate probability distributions. A good agreement between analytical and numerical values of the first order and total indices for considered test cases is obtained. The behavior of sensitivity indices depends on the relative predominance of interactions and correlations. The method is shown to be efficient and general.},
	language = {en},
	number = {4},
	urldate = {2020-04-19},
	journal = {Computer Physics Communications},
	author = {Kucherenko, S. and Tarantola, S. and Annoni, P.},
	month = apr,
	year = {2012},
	keywords = {Global sensitivity analysis, Correlated inputs, Gaussian copula, Quasi Monte Carlo methods, Sobolʼ sensitivity indices, Sobolʼ sequences},
	pages = {937--946},
	file = {ScienceDirect Snapshot:/Users/bbush/Zotero/storage/RWEVUF2Z/S0010465511004085.html:text/html;ScienceDirect Full Text PDF:/Users/bbush/Zotero/storage/24XDM6AP/Kucherenko et al. - 2012 - Estimation of global sensitivity indices for model.pdf:application/pdf}
}


@book{saltelli_sensitivity_2004,
	address = {Hoboken, NJ},
	title = {Sensitivity analysis in practice: a guide to assessing scientific models},
	isbn = {978-0-470-87093-8},
	shorttitle = {Sensitivity analysis in practice},
	language = {en},
	publisher = {Wiley},
	editor = {Saltelli, A.},
	year = {2004},
	keywords = {Sensitivity theory (Mathematics), Simulation methods, SIMLAB},
	file = {Saltelli - 2004 - Sensitivity analysis in practice a guide to asses.pdf:/Users/bbush/Zotero/storage/DT4TQFY4/Saltelli - 2004 - Sensitivity analysis in practice a guide to asses.pdf:application/pdf}
}


@book{saltelli_global_2008,
	address = {Chichester, England ; Hoboken, NJ},
	title = {Global sensitivity analysis: the primer},
	isbn = {978-0-470-05997-5},
	shorttitle = {Global sensitivity analysis},
	language = {en},
	publisher = {John Wiley},
	editor = {Saltelli, A.},
	year = {2008},
	note = {OCLC: ocn180852094},
	keywords = {Global analysis (Mathematics), Mathematical models, Sensitivity theory (Mathematics)},
	file = {Saltelli - 2008 - Global sensitivity analysis the primer.pdf:/Users/bbush/Zotero/storage/Y3R27KPC/Saltelli - 2008 - Global sensitivity analysis the primer.pdf:application/pdf}
}


@article{wei_regional_2014,
	title = {Regional sensitivity analysis using revised mean and variance ratio functions},
	volume = {121},
	issn = {0951-8320},
	url = {http://www.sciencedirect.com/science/article/pii/S0951832013002366},
	doi = {10.1016/j.ress.2013.08.001},
	abstract = {The variance ratio function, derived from the contribution to sample variance (CSV) plot, is a regional sensitivity index for studying how much the output deviates from the original mean of model output when the distribution range of one input is reduced and to measure the contribution of different distribution ranges of each input to the variance of model output. In this paper, the revised mean and variance ratio functions are developed for quantifying the actual change of the model output mean and variance, respectively, when one reduces the range of one input. The connection between the revised variance ratio function and the original one is derived and discussed. It is shown that compared with the classical variance ratio function, the revised one is more suitable to the evaluation of model output variance due to reduced ranges of model inputs. A Monte Carlo procedure, which needs only a set of samples for implementing it, is developed for efficiently computing the revised mean and variance ratio functions. The revised mean and variance ratio functions are compared with the classical ones by using the Ishigami function. At last, they are applied to a planar 10-bar structure.},
	language = {en},
	urldate = {2020-12-21},
	journal = {Reliability Engineering \& System Safety},
	author = {Wei, Pengfei and Lu, Zhenzhou and Ruan, Wenbin and Song, Jingwen},
	month = jan,
	year = {2014},
	keywords = {Mean ratio function, Regional sensitivity analysis, Variance ratio function, Variance reduction},
	pages = {121--135},
	file = {ScienceDirect Full Text PDF:/Users/bbush/Zotero/storage/ZJBQDL4I/Wei et al. - 2014 - Regional sensitivity analysis using revised mean a.pdf:application/pdf;ScienceDirect Snapshot:/Users/bbush/Zotero/storage/MTKLBPDC/S0951832013002366.html:text/html}
}


@article{gamboa_sensitivity_2013,
	title = {Sensitivity indices for multivariate outputs},
	volume = {351},
	issn = {1631-073X},
	url = {http://www.sciencedirect.com/science/article/pii/S1631073X13000824},
	doi = {10.1016/j.crma.2013.04.016},
	abstract = {We define and study a generalization of Sobol sensitivity indices for the case of a vector output.
Résumé
Nous définissons et étudions une généralisation des indices de Sobol pour des sorties vectorielles.},
	language = {en},
	number = {7},
	urldate = {2020-12-21},
	journal = {Comptes Rendus Mathematique},
	author = {Gamboa, Fabrice and Janon, Alexandre and Klein, Thierry and Lagnoux, Agnès},
	month = apr,
	year = {2013},
	pages = {307--310},
	file = {Submitted Version:/Users/bbush/Zotero/storage/2SRI3FF5/Gamboa et al. - 2013 - Sensitivity indices for multivariate outputs.pdf:application/pdf;ScienceDirect Snapshot:/Users/bbush/Zotero/storage/SRI6Y4S8/S1631073X13000824.html:text/html}
}

@article{wagener_what_2019,
	title = {What has {Global} {Sensitivity} {Analysis} ever done for us? {A} systematic review to support scientific advancement and to inform policy-making in earth system modelling},
	volume = {194},
	issn = {0012-8252},
	shorttitle = {What has {Global} {Sensitivity} {Analysis} ever done for us?},
	url = {http://www.sciencedirect.com/science/article/pii/S0012825218300990},
	doi = {10.1016/j.earscirev.2019.04.006},
	abstract = {Computer models are essential tools in the earth system sciences. They underpin our search for understanding of earth system functioning and support decision- and policy-making across spatial and temporal scales. To understand the implications of uncertainty and environmental variability on the identification of such earth system models and their predictions, we can rely on increasingly powerful Global Sensitivity Analysis (GSA) methods. Previous reviews have characterised the variability of GSA methods available and their usability for different tasks. In our paper we rather focus on reviewing what has been learned so far by applying GSA to models across the earth system sciences, independently of the specific algorithm that was applied. We identify and discuss 10 key findings with general applicability and relevance for the earth sciences. We further provide an A-B-C-D of best practise in applying GSA methods, which we have derived from analysing why some GSA applications provided more insight than others.},
	language = {en},
	urldate = {2020-12-21},
	journal = {Earth-Science Reviews},
	author = {Wagener, Thorsten and Pianosi, Francesca},
	month = jul,
	year = {2019},
	pages = {1--18},
	file = {ScienceDirect Full Text PDF:/Users/bbush/Zotero/storage/AMZXHDA9/Wagener and Pianosi - 2019 - What has Global Sensitivity Analysis ever done for.pdf:application/pdf;ScienceDirect Snapshot:/Users/bbush/Zotero/storage/TKQEV8LB/S0012825218300990.html:text/html}
}


@article{wu_application_2017,
  series = {{SI}: {Advanced} {Transportation} {Infrastructure} and {Materials}},
  title = {Application of {Monte} {Carlo} filtering method in regional sensitivity analysis of {AASHTOWare} {Pavement} {ME} design},
  volume = {4},
  issn = {2095-7564},
  url = {http://www.sciencedirect.com/science/article/pii/S2095756417300934},
  doi = {10.1016/j.jtte.2017.03.006},
  abstract = {Since AASHTO released the Mechanistic-Empirical Pavement Design Guide (MEPDG) for public review in 2004, many highway research agencies have performed sensitivity analyses using the prototype MEPDG design software. The information provided by the sensitivity analysis is essential for design engineers to better understand the MEPDG design models and to identify important input parameters for pavement design. In literature, different studies have been carried out based on either local or global sensitivity analysis methods, and sensitivity indices have been proposed for ranking the importance of the input parameters. In this paper, a regional sensitivity analysis method, Monte Carlo filtering (MCF), is presented. The MCF method maintains many advantages of the global sensitivity analysis, while focusing on the regional sensitivity of the MEPDG model near the design criteria rather than the entire problem domain. It is shown that the information obtained from the MCF method is more helpful and accurate in guiding design engineers in pavement design practices. To demonstrate the proposed regional sensitivity method, a typical three-layer flexible pavement structure was analyzed at input level 3. A detailed procedure to generate Monte Carlo runs using the AASHTOWare Pavement ME Design software was provided. The results in the example show that the sensitivity ranking of the input parameters in this study reasonably matches with that in a previous study under a global sensitivity analysis. Based on the analysis results, the strengths, practical issues, and applications of the MCF method were further discussed.},
  language = {en},
  number = {2},
  urldate = {2020-12-22},
  journal = {Journal of Traffic and Transportation Engineering (English Edition)},
  author = {Wu, Zhong and Yang, Xiaoming and Sun, Xiaohui},
  month = apr,
  year = {2017},
  keywords = {MEPDG, Monte Carlo filtering, Pavement design, Sensitivity analysis},
  pages = {185--197},
  file = {ScienceDirect Full Text PDF:/Users/bbush/Zotero/storage/FTUXILL5/Wu et al. - 2017 - Application of Monte Carlo filtering method in reg.pdf:application/pdf;ScienceDirect Snapshot:/Users/bbush/Zotero/storage/MQT38Q9F/S2095756417300934.html:text/html}
}


@article{rose_parameter_1991,
  title = {Parameter sensitivities, monte carlo filtering, and model forecasting under uncertainty},
  volume = {10},
  copyright = {Copyright © 1991 John Wiley \& Sons, Ltd.},
  issn = {1099-131X},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/for.3980100108},
  doi = {https://doi.org/10.1002/for.3980100108},
  abstract = {Complex models are often used to make predictions of environmental effects over a broad range of temporal and spatial scales. The data necessary to adequately estimate the parameters of these complex models are often not available. Monte Carlo filtering, the process of rejecting sets of mode! simulations that fail to meet prespecified criteria of model performance, is a useful procedure for objectively establishing parameter values and improving confidence in model predictions. This paper uses a foodweb model to examine the relationship between model sensitivities and Monte Carlo filtering. Results show that Monte Carlo filtering with a behavior definition that is closely related to the sensitivity structure of the model will produce substantial reductions in model forecasting uncertainty.},
  language = {en},
  number = {1-2},
  urldate = {2020-12-22},
  journal = {Journal of Forecasting},
  author = {Rose, Kenneth A. and Smith, Eric P. and Gardner, Robert H. and Brenkert, Antoinette L. and Bartell, Steven M.},
  year = {1991},
  note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/for.3980100108},
  keywords = {Calibration Parameter estimation, Monte Carlo filtering, Parameter sensitivity, Prediction uncertainty},
  pages = {117--133},
  file = {Snapshot:/Users/bbush/Zotero/storage/FC2KC6QN/for.html:text/html}
}


@article{lu_non-uniform_2020,
  title = {Non-uniform space filling ({NUSF}) designs},
  volume = {0},
  issn = {0022-4065},
  url = {https://doi.org/10.1080/00224065.2020.1727801},
  doi = {10.1080/00224065.2020.1727801},
  abstract = {Space-filling designs are a convenient and effective approach for exploring the input space for experiments. However, standard choices for these designs strive to provide uniform density of points throughout the region of interest. There are numerous situations where flexibility to adapt the density of points to match specific design objectives would be advantageous to maximize the efficiency of the design. In this paper, we propose non-uniform space-filling (NUSF) designs to achieve a user-specified desired density distribution of design points across the input space and demonstrate how to implement NUSF designs in different ways to provide the experimenters with flexibility to match their goals. The approach is flexible for a variety of scenarios where the experimenter wishes to control the density of points throughout the region while still preserving the space-filling characteristic. Details are provided about how to translate a problem into an appropriate weight structure to generate several designs which can then be compared using graphical methods, including the Closest Distance by Weight plot, to determine if the desired characteristics have been achieved. The methods are demonstrated with two real examples with different requirements for design point placement.},
  number = {0},
  urldate = {2020-12-22},
  journal = {Journal of Quality Technology},
  author = {Lu, Lu and Anderson-Cook, Christine M. and Ahmed, Towfiq},
  month = feb,
  year = {2020},
  note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/00224065.2020.1727801},
  keywords = {Closest Distance by Weight plot, computer experiments, data competitions, point exchange, unequal density of design points, weighted distance metric},
  pages = {1--22},
  file = {Snapshot:/Users/bbush/Zotero/storage/YEXFCFM8/00224065.2020.html:text/html}
}

@article{bowman_weighted_2013,
  title = {Weighted space-filling designs},
  volume = {7},
  issn = {1747-7778},
  url = {https://doi.org/10.1057/jos.2013.8},
  doi = {10.1057/jos.2013.8},
  abstract = {Many computer models or simulators have probabilistic dependencies between their input variables, which if not accounted for during design selection may result in a large numbers of simulator runs being required for analysis. We propose a method that incorporates known dependencies between input variables into design selection for simulators and demonstrate the benefits of this approach via a simulator for atmospheric dispersion. We quantify the benefit of the new techniques over standard space-filling and Monte Carlo simulation. The proposed methods are adaptations of computer-generated spread and coverage space-filling designs, with ‘distance’ between two input points redefined to include a weight function. This weight function reflects any known multivariate dependencies between input variables and prior information on the design region. The methods can include quantitative and qualitative variables, and different types of prior information. Novel graphical methods, adapted from fraction of design space plots, are used to assess and compare the designs.},
  number = {4},
  urldate = {2020-12-22},
  journal = {Journal of Simulation},
  author = {Bowman, Veronica E. and Woods, David C.},
  month = nov,
  year = {2013},
  note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1057/jos.2013.8},
  keywords = {computer experiments, defence studies, design of experiments, simulation experiments},
  pages = {249--263},
  file = {Snapshot:/Users/bbush/Zotero/storage/MKX35MGE/jos.2013.html:text/html;Accepted Version:/Users/bbush/Zotero/storage/ZCG34965/Bowman and Woods - 2013 - Weighted space-filling designs.pdf:application/pdf;Snapshot:/Users/bbush/Zotero/storage/FJQIPLC2/jos.2013.html:text/html}
}


@article{myers_partitioning_2016,
  title = {Partitioning a {Large} {Simulation} as {It} {Runs}},
  volume = {58},
  issn = {0040-1706},
  url = {https://doi.org/10.1080/00401706.2016.1158740},
  doi = {10.1080/00401706.2016.1158740},
  abstract = {As computer simulations continue to grow in size and complexity, they present a particularly challenging class of big data problems. Many application areas are moving toward exascale computing systems, systems that perform 1018 FLOPS (FLoating-point Operations Per Second)—a billion billion calculations per second. Simulations at this scale can generate output that exceeds both the storage capacity and the bandwidth available for transfer to storage, making post-processing and analysis challenging. One approach is to embed some analyses in the simulation while the simulation is running—a strategy often called in situ analysis—to reduce the need for transfer to storage. Another strategy is to save only a reduced set of time steps rather than the full simulation. Typically the selected time steps are evenly spaced, where the spacing can be defined by the budget for storage and transfer. This article combines these two ideas to introduce an online in situ method for identifying a reduced set of time steps of the simulation to save. Our approach significantly reduces the data transfer and storage requirements, and it provides improved fidelity to the simulation to facilitate post-processing and reconstruction. We illustrate the method using a computer simulation that supported NASA's 2009 Lunar Crater Observation and Sensing Satellite mission.},
  number = {3},
  urldate = {2020-12-22},
  journal = {Technometrics},
  author = {Myers, Kary and Lawrence, Earl and Fugate, Michael and Bowen, Claire McKay and Ticknor, Lawrence and Woodring, Jon and Wendelberger, Joanne and Ahrens, Jim},
  month = jul,
  year = {2016},
  note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/00401706.2016.1158740},
  keywords = {Change-point detection, Complex computer models, Exascale computing, Online methods, Piecewise linear fitting, Streaming data.},
  pages = {329--340},
  file = {Snapshot:/Users/bbush/Zotero/storage/JVJCMT4H/00401706.2016.html:text/html;Submitted Version:/Users/bbush/Zotero/storage/2JH94Q5Q/Myers et al. - 2016 - Partitioning a Large Simulation as It Runs.pdf:application/pdf;Snapshot:/Users/bbush/Zotero/storage/D8VVZ2PH/00401706.2016.html:text/html}
}


@inproceedings{hanes_supporting_2020,
	address = {United States},
	title = {Supporting {Bioproducts} {Industry} {Growth} with a {System} {Dynamics} {Decision}-{Support} {Tool}},
	url = {https://www.osti.gov/biblio/1669591},
	abstract = {A bio-based economy requires chemical products as well as fuels to be produced from biomass. Although a variety of universities, government agencies, start-ups and established firms have engaged in bioproduct development, many projects have failed to reach the point of commercialization and commercialized bioproducts struggle to capture and maintain market share. To date, there has been no general research into the factors that contribute to bioproduct failure or success. This work presents the Bioproduct Transition Dynamics (BTD) system dynamics model, a decision-support tool that simulates the bioproduct development process from pre-piloting research through construction of the first commercial-scale plant, as well as the processes of obtaining funding from investors and government agencies. The core of the BTD is a feedback loop between bioproduct developers and funders, which relates development progress measured with indicators such as net present value to funders’ decisions to continue investing. External factors such as feedstock prices, market size and growth, and the existence of bioproduct consumers also influence a development project’s chances of receiving follow-on funding. Virtually any bioproduct can be represented with the BTD: direct replacements, performance-advantaged products, niche and commodity markets can all be modeled. The goal of the BTD project is to inform decisions made by developers in both established firms and start-ups, investors, government agencies, and other stakeholders interested in growing the nascent U.S. bioproducts industry. This talk will cover the general structure and functionality of the BTD, and present results from an analysis performed with the BTD to demonstrate its use as a decision-support tool and the insights it can provide. The goal of the analysis is to identify the most critical factors that lead to direct replacement and performance-advantaged bioproduct projects emerging successfully from the “Valley of Death”, and to determine if these factors differ between the two bioproduct types.},
	language = {English},
	author = {Hanes, Rebecca and Sittler, Lauren and Bush, Brian},
	month = sep,
	year = {2020},
	note = {CONF-},
	keywords = {system dynamics, decision support, 29 EE - Bioenergy Technologies Office (EE-3B), bioeconomy, bioproducts industry, v2}
}

@inproceedings{hanes_transition_2019,
	address = {United States},
	title = {Transition {Dynamics} in the {Nascent} {Bioproducts} {Industry}},
	url = {https://www.osti.gov/biblio/1548266},
	abstract = {Bioproducts are chemicals derived from a biomass feedstock that can be produced on their own or as biofuel co-products. Historically, bioproduct developers have faced significant barriers in scaling up bioproduct technologies and in bringing bioproducts to market, resulting in a high failure rate for bioproduct development projects. To date, there has been essentially no research done on the factors that lead to a bioproduct successfully capturing market share and why bioproduct projects fail. We address this knowledge gap with a model of the transition dynamics involved in taking a bioproduct technology from applied research to commercial production and the interactions between bioproduct developers, investors providing private funds, and federal government agencies which may provide cost-sharing and production incentives. This model incorporates several novel techniques including stochastic submodels and forecasts of market shares and company performance. Our research objectives are to identify relevant factors and mechanisms that impact bioproduct success and to develop a decision support tool, the Bioproduct Transition Dynamics (BTD) system dynamics model, that can be used by investors, bioproduct developers and government agencies to bring more bioproducts to market and to grow the nascent bioproducts industry.},
	language = {English},
	author = {Hanes, Rebecca and Bush, Brian W},
	month = aug,
	year = {2019},
	note = {CONF-},
	keywords = {investment, 29 ENERGY PLANNING, POLICY, AND ECONOMY, 32 ENERGY CONSERVATION, CONSUMPTION, AND UTILIZATION, bioproducts, commercial production, market share, technology development, valley of death, v2}
}

@inproceedings{hanes_introduction_2018,
	address = {United States},
	title = {Introduction to the {Bioproduct} {Transition} {Dynamics} {Model}},
	url = {https://www.osti.gov/biblio/1458910},
	abstract = {The U.S. Department of Energy, Bioenergy Technologies Office has a broad understanding of different conversion processes that produce bioproducts (chemicals derived from biomass feedstocks) and the market attributes of those processes. However, bioproducts have in many cases proven to be difficult to scale up to commercial production and bring to market, and there is currently a need for greater understanding around the possible successful scenarios for advancing the bioproducts industry. Insight into the myriad factors that impact the success of individual bioproducts and the growth of the bioproducts industry will enable bioproduct stakeholders to evaluate strategies for development and investment that could achieve the greatest impact. An understanding of these factors will also enable the identification of synergies between the bioproducts and biofuels industries that occur through shared learning and co-production. The Bioproduct Transition Dynamics (BTD) model explores questions such as the following: what are the factors that separate a successful, commercially produced bioproduct like succinic acid from one that never progresses beyond bench-scale lab research? Can these factors be influenced by stakeholders, and to what extent? The BTD model uses system dynamics to capture the impacts of investor decision-making, bioproduct techno-economics, and market factors during the early stages of bioproduct development. Key components of the BTD model include techno-economic benchmarks that inform the investor decision-making process, failures and setbacks in development and the resulting need for additional work, and the impacts of effective or ineffective management during each development stage. These factors and their interactions are tracked through bench-scale laboratory research, piloting, demoing, and the construction and operation of the first commercial-scale plant. Bioproduct development is linked to the biofuels industry through a shared-learning model, enabling the benefit to biofuels from bioproduct technology and feedstock supply chain development to be quantified. A wide variety of bioproducts can be analyzed using the BTD model, including bioproducts that have been developed through economic or policy-driven mechanisms, and those with either niche (low volume) or scalable (high volume) markets. The BTD model also has the potential to apply to technology development processes outside the bioproducts industry; background research performed for this project has shown that the early stages of technology development are similar, and impacted by similar factors, regardless of the exact industry. This presentation will cover the BTD model logic, input data, validation process, and results of a sensitivity analysis based on techno-economic data for succinic acid. The sensitivity analysis was performed to identify factors with the largest impact on the eventual success or failure of a bioproduct. Results indicate that management effectiveness, which affects how efficiently money spent is converted into technological advancements, is the single most important factor in determining whether a bioproduct reaches commercial-scale production. The capacity of the first commercial plant and the type of biomass feedstock used, as well as the presence of government support in the form of cost-sharing, were also identified as significant factors.},
	language = {English},
	publisher = {National Renewable Energy Lab. (NREL), Golden, CO (United States)},
	author = {Hanes, Rebecca and Bush, Brian W and Newes, Emily K},
	month = jun,
	year = {2018},
	keywords = {system dynamics, 29 ENERGY PLANNING, POLICY, AND ECONOMY, bioproducts, technology development, v2}
}

@inproceedings{hanes_system_2018,
	address = {United States},
	title = {A {System} {Dynamics} {Model} of {Early}-{Stage} {Transition} {Dynamics} in the {Bioproducts} {Industry}},
	url = {https://www.osti.gov/biblio/1458911},
	abstract = {The U.S. Department of Energy, Bioenergy Technologies Office has a broad understanding of different conversion processes that produce bioproducts (chemicals derived from biomass feedstocks) and the techno-economic attributes of those processes. However, bioproducts have in many cases proven to be difficult to scale up to commercial production, and there is currently a need for greater understanding around the possible successful scenarios for advancing the bioproducts industry. Insight into the myriad factors that impact the success of individual bioproducts and the growth of the bioproducts industry will enable bioproduct stakeholders to evaluate strategies for development and investment that could achieve the greatest impact. An understanding of these factors will also enable the identification of synergies between the bioproducts and biofuels industries that occur through shared learning and co-production. The Bioproduct Transition Dynamics (BTD) model explores questions such as the following: what are the factors that separate a successful, commercially produced bioproduct like succinic acid from one that never progresses beyond bench-scale lab research? Can these factors be influenced by stakeholders, and to what extent? The BTD model uses system dynamics to capture the impacts of investor decision-making, bioproduct techno-economics, and end use factors during the early stages of bioproduct development. Key components of the BTD model include techno-economic benchmarks that inform the investor decision-making process, failures and setbacks in development and the resulting need for additional work, and the impacts of effective or ineffective management during each development stage. These factors and their interactions are tracked through bench-scale laboratory research, piloting, demoing, and the construction and operation of the first commercial-scale plant. Bioproduct development is linked to the biofuels industry through a shared-learning model, enabling the benefit to biofuels from bioproduct technology and feedstock supply chain development to be quantified. A wide variety of bioproducts can be analyzed using the BTD model, including bioproducts that have been developed through economic or policy-driven mechanisms, and those with either niche (low volume) or scalable (high volume) demand. The BTD model also has the potential to apply to technology development processes outside the bioproducts industry; background research performed for this project has shown that the early stages of technology development are similar, and impacted by similar factors, regardless of the exact industry. This presentation will cover the BTD model logic, input data, validation process, and results of a sensitivity analysis based on techno-economic data for succinic acid. The sensitivity analysis was performed to identify factors with the largest impact on the eventual success or failure of a bioproduct. Results indicate that management effectiveness, which affects how efficiently money spent is converted into technological advancements, is the single most important factor in determining whether a bioproduct reaches commercial-scale production. The capacity of the first commercial plant and the type of biomass feedstock used, as well as the presence of government support in the form of cost-sharing, were also identified as significant factors.},
	language = {English},
	publisher = {National Renewable Energy Lab. (NREL), Golden, CO (United States)},
	author = {Hanes, Rebecca and Bush, Brian W and Newes, Emily K},
	month = jun,
	year = {2018},
	note = {CONF-},
	keywords = {system dynamics, 29 ENERGY PLANNING, POLICY, AND ECONOMY, bioproducts, technology development, v2}
}


@book{ramsay_functional_2013,
  title = {Functional {Data} {Analysis}},
  isbn = {978-1-4757-7107-7},
  abstract = {Scientists today collect samples of curves and other functional observations. This monograph presents many ideas and techniques for such data. Included are expressions in the functional domain of such classics as linear regression, principal components analysis, linear modelling, and canonical correlation analysis, as well as specifically functional techniques such as curve registration and principal differential analysis. Data arising in real applications are used throughout for both motivation and illustration, showing how functional approaches allow us to see new things, especially by exploiting the smoothness of the processes generating the data. The data sets exemplify the wide scope of functional data analysis; they are drwan from growth analysis, meterology, biomechanics, equine science, economics, and medicine. The book presents novel statistical technology while keeping the mathematical level widely accessible. It is designed to appeal to students, to applied data analysts, and to experienced researchers; it will have value both within statistics and across a broad spectrum of other fields. Much of the material is based on the authors' own work, some of which appears here for the first time. Jim Ramsay is Professor of Psychology at McGill University and is an international authority on many aspects of multivariate analysis. He draws on his collaboration with researchers in speech articulation, motor control, meteorology, psychology, and human physiology to illustrate his technical contributions to functional data analysis in a wide range of statistical and application journals. Bernard Silverman, author of the highly regarded "Density Estimation for Statistics and Data Analysis," and coauthor of "Nonparametric Regression and Generalized Linear Models: A Roughness Penalty Approach," is Professor of Statistics at Bristol University. His published work on smoothing methods and other aspects of applied, computational, and theoretical statistics has been recognized by the Presidents' Award of the Committee of Presidents of Statistical Societies, and the award of two Guy Medals by the Royal Statistical Society.},
  language = {en},
  publisher = {Springer Science \& Business Media},
  author = {Ramsay, James and Silverman, B. W.},
  month = nov,
  year = {2013},
  note = {Google-Books-ID: fgLqBwAAQBAJ},
  keywords = {Mathematics / Probability \& Statistics / General, Mathematics / Probability \& Statistics / Stochastic Processes}
}

@article{bugbee_enabling_2019,
  title = {Enabling immersive engagement in energy system models with deep learning},
  volume = {12},
  copyright = {© 2019 The Authors. Statistical Analysis and Data Mining: The ASA Data Science Journal published by Wiley Periodicals, Inc.},
  issn = {1932-1872},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sam.11419},
  doi = {https://doi.org/10.1002/sam.11419},
  abstract = {Complex ensembles of energy simulation models have become significant components of renewable energy research in recent years. Often the significant computational cost, high-dimensional structure, and other complexities hinder researchers from fully utilizing these data sources for knowledge building. Researchers at National Renewable Energy Laboratory have developed an immersive visualization workflow to dramatically improve user engagement and analysis capability through a combination of low-dimensional structure analysis, deep learning, and custom visualization methods. We present case studies for two energy simulation platforms.},
  language = {en},
  number = {4},
  urldate = {2020-12-23},
  journal = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  author = {Bugbee, Bruce and Bush, Brian W. and Gruchalla, Kenny and Potter, Kristin and Brunhart‐Lupo, Nicholas and Krishnan, Venkat},
  year = {2019},
  note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sam.11419},
  keywords = {high-dimensional data, interactive visualization, neural networks, renewable energy, t-SNE, Tucker decomposition},
  pages = {325--337},
  file = {Full Text PDF:/Users/bbush/Zotero/storage/B77GKXMU/Bugbee et al. - 2019 - Enabling immersive engagement in energy system mod.pdf:application/pdf;Snapshot:/Users/bbush/Zotero/storage/D342ZYG4/sam.html:text/html}
}


@book{box_statistics_2005,
  title = {Statistics for {Experimenters}: {Design}, {Innovation}, and {Discovery}},
  isbn = {978-0-471-71813-0},
  shorttitle = {Statistics for {Experimenters}},
  abstract = {A Classic adapted to modern times Rewritten and updated, this new edition of Statistics for Experimenters adopts the same approaches as the landmark First Edition by teaching with examples, readily understood graphics, and the appropriate use of computers. Catalyzing innovation, problem solving, and discovery, the Second Edition provides experimenters with the scientific and statistical tools needed to maximize the knowledge gained from research data, illustrating how these tools may best be utilized during all stages of the investigative process. The authors’ practical approach starts with a problem that needs to be solved and then examines the appropriate statistical methods of design and analysis. Providing even greater accessibility for its users, the Second Edition is thoroughly revised and updated to reflect the changes in techniques and technologies since the publication of the classic First Edition. Among the new topics included are:  Graphical Analysis of Variance Computer Analysis of Complex Designs Simplification by transformation Hands-on experimentation using Response Service Methods Further development of robust product and process design using split plot arrangements and minimization of error transmission Introduction to Process Control, Forecasting and Time Series Illustrations demonstrating how multi-response problems can be solved using the concepts of active and inert factor spaces and canonical spaces Bayesian approaches to model selection and sequential experimentation  An appendix featuring Quaquaversal quotes from a variety of sources including noted statisticians and scientists to famous philosophers is provided to illustrate key concepts and enliven the learning process. All the computations in the Second Edition can be done utilizing the statistical language R. Functions for displaying ANOVA and lamba plots, Bayesian screening, and model building are all included and R packages are available online. All theses topics can also be applied utilizing easy-to-use commercial software packages. Complete with applications covering the physical, engineering, biological, and social sciences, Statistics for Experimenters is designed for individuals who must use statistical approaches to conduct an experiment, but do not necessarily have formal training in statistics. Experimenters need only a basic understanding of mathematics to master all the statistical methods presented. This text is an essential reference for all researchers and is a highly recommended course book for undergraduate and graduate students.},
  language = {en},
  publisher = {Wiley},
  author = {Box, George E. P. and Hunter, J. Stuart and Hunter, William G.},
  month = may,
  year = {2005},
  note = {Google-Books-ID: oYUpAQAAMAAJ},
  keywords = {Mathematics / Probability \& Statistics / General, Mathematics / Probability \& Statistics / Stochastic Processes, Science / Research \& Methodology, Technology \& Engineering / Industrial Engineering}
}

